{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다중 선형 회귀 실습\n",
    "# 앞서 배운 x가 1개인 선형 회귀 -> 단순 선형 이라고합니다.\n",
    "# 다수 x 로부터 y를 예측하는 다중 선형 회귀\n",
    "x1_train = torch.FloatTensor([[73], [93], [83], [96], [73]])\n",
    "x2_train = torch.FloatTensor([[80], [88], [91], [98], [66]])\n",
    "x3_train = torch.FloatTensor([[75], [93], [90], [100], [70]])\n",
    "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가중치 w 와 편향 b를 선언 필요하고 w -> 3개 b -> 1개\n",
    "w1 = torch.zeros(1, requires_grad=True)\n",
    "w2 = torch.zeros(1, requires_grad=True)\n",
    "w3 = torch.zeros(1, requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer 1e-04 - 0.0001 0.00001\n",
    "optimizer = optim.SGD([w1, w2, w3, b], lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/10000 w1 0.290 w2 0.294 w3 0.297 b 0.003 loss 29661.800781\n",
      "Epoch  100/10000 w1 0.666 w2 0.671 w3 0.683 b 0.008 loss 2.596959\n",
      "Epoch  200/10000 w1 0.667 w2 0.669 w3 0.685 b 0.008 loss 2.588804\n",
      "Epoch  300/10000 w1 0.668 w2 0.667 w3 0.686 b 0.008 loss 2.581123\n",
      "Epoch  400/10000 w1 0.669 w2 0.664 w3 0.687 b 0.008 loss 2.573900\n",
      "Epoch  500/10000 w1 0.670 w2 0.662 w3 0.688 b 0.008 loss 2.567041\n",
      "Epoch  600/10000 w1 0.671 w2 0.660 w3 0.689 b 0.008 loss 2.560563\n",
      "Epoch  700/10000 w1 0.672 w2 0.658 w3 0.690 b 0.009 loss 2.554451\n",
      "Epoch  800/10000 w1 0.673 w2 0.656 w3 0.692 b 0.009 loss 2.548648\n",
      "Epoch  900/10000 w1 0.673 w2 0.654 w3 0.693 b 0.009 loss 2.543140\n",
      "Epoch 1000/10000 w1 0.674 w2 0.652 w3 0.694 b 0.009 loss 2.537928\n",
      "Epoch 1100/10000 w1 0.675 w2 0.651 w3 0.695 b 0.009 loss 2.532964\n",
      "Epoch 1200/10000 w1 0.675 w2 0.649 w3 0.696 b 0.009 loss 2.528231\n",
      "Epoch 1300/10000 w1 0.676 w2 0.647 w3 0.697 b 0.009 loss 2.523768\n",
      "Epoch 1400/10000 w1 0.677 w2 0.646 w3 0.698 b 0.009 loss 2.519482\n",
      "Epoch 1500/10000 w1 0.677 w2 0.644 w3 0.699 b 0.009 loss 2.515377\n",
      "Epoch 1600/10000 w1 0.677 w2 0.642 w3 0.700 b 0.010 loss 2.511494\n",
      "Epoch 1700/10000 w1 0.678 w2 0.641 w3 0.701 b 0.010 loss 2.507757\n",
      "Epoch 1800/10000 w1 0.678 w2 0.639 w3 0.703 b 0.010 loss 2.504210\n",
      "Epoch 1900/10000 w1 0.679 w2 0.638 w3 0.704 b 0.010 loss 2.500776\n",
      "Epoch 2000/10000 w1 0.679 w2 0.636 w3 0.705 b 0.010 loss 2.497503\n",
      "Epoch 2100/10000 w1 0.679 w2 0.635 w3 0.706 b 0.010 loss 2.494367\n",
      "Epoch 2200/10000 w1 0.680 w2 0.634 w3 0.707 b 0.010 loss 2.491338\n",
      "Epoch 2300/10000 w1 0.680 w2 0.632 w3 0.708 b 0.010 loss 2.488410\n",
      "Epoch 2400/10000 w1 0.680 w2 0.631 w3 0.709 b 0.010 loss 2.485591\n",
      "Epoch 2500/10000 w1 0.680 w2 0.630 w3 0.710 b 0.010 loss 2.482870\n",
      "Epoch 2600/10000 w1 0.680 w2 0.629 w3 0.711 b 0.011 loss 2.480267\n",
      "Epoch 2700/10000 w1 0.681 w2 0.627 w3 0.712 b 0.011 loss 2.477753\n",
      "Epoch 2800/10000 w1 0.681 w2 0.626 w3 0.713 b 0.011 loss 2.475293\n",
      "Epoch 2900/10000 w1 0.681 w2 0.625 w3 0.714 b 0.011 loss 2.472928\n",
      "Epoch 3000/10000 w1 0.681 w2 0.624 w3 0.715 b 0.011 loss 2.470619\n",
      "Epoch 3100/10000 w1 0.681 w2 0.623 w3 0.716 b 0.011 loss 2.468392\n",
      "Epoch 3200/10000 w1 0.681 w2 0.622 w3 0.717 b 0.011 loss 2.466215\n",
      "Epoch 3300/10000 w1 0.681 w2 0.621 w3 0.718 b 0.011 loss 2.464108\n",
      "Epoch 3400/10000 w1 0.681 w2 0.620 w3 0.719 b 0.011 loss 2.462019\n",
      "Epoch 3500/10000 w1 0.681 w2 0.619 w3 0.720 b 0.012 loss 2.460030\n",
      "Epoch 3600/10000 w1 0.681 w2 0.618 w3 0.721 b 0.012 loss 2.458078\n",
      "Epoch 3700/10000 w1 0.681 w2 0.617 w3 0.722 b 0.012 loss 2.456162\n",
      "Epoch 3800/10000 w1 0.681 w2 0.616 w3 0.723 b 0.012 loss 2.454310\n",
      "Epoch 3900/10000 w1 0.681 w2 0.615 w3 0.724 b 0.012 loss 2.452471\n",
      "Epoch 4000/10000 w1 0.681 w2 0.614 w3 0.725 b 0.012 loss 2.450689\n",
      "Epoch 4100/10000 w1 0.681 w2 0.613 w3 0.726 b 0.012 loss 2.448940\n",
      "Epoch 4200/10000 w1 0.680 w2 0.612 w3 0.727 b 0.012 loss 2.447216\n",
      "Epoch 4300/10000 w1 0.680 w2 0.612 w3 0.728 b 0.012 loss 2.445529\n",
      "Epoch 4400/10000 w1 0.680 w2 0.611 w3 0.729 b 0.012 loss 2.443883\n",
      "Epoch 4500/10000 w1 0.680 w2 0.610 w3 0.730 b 0.013 loss 2.442250\n",
      "Epoch 4600/10000 w1 0.680 w2 0.609 w3 0.731 b 0.013 loss 2.440670\n",
      "Epoch 4700/10000 w1 0.680 w2 0.608 w3 0.732 b 0.013 loss 2.439113\n",
      "Epoch 4800/10000 w1 0.679 w2 0.608 w3 0.733 b 0.013 loss 2.437563\n",
      "Epoch 4900/10000 w1 0.679 w2 0.607 w3 0.734 b 0.013 loss 2.436041\n",
      "Epoch 5000/10000 w1 0.679 w2 0.606 w3 0.735 b 0.013 loss 2.434542\n",
      "Epoch 5100/10000 w1 0.679 w2 0.605 w3 0.736 b 0.013 loss 2.433060\n",
      "Epoch 5200/10000 w1 0.679 w2 0.605 w3 0.737 b 0.013 loss 2.431609\n",
      "Epoch 5300/10000 w1 0.678 w2 0.604 w3 0.738 b 0.013 loss 2.430164\n",
      "Epoch 5400/10000 w1 0.678 w2 0.603 w3 0.738 b 0.014 loss 2.428756\n",
      "Epoch 5500/10000 w1 0.678 w2 0.603 w3 0.739 b 0.014 loss 2.427361\n",
      "Epoch 5600/10000 w1 0.678 w2 0.602 w3 0.740 b 0.014 loss 2.425954\n",
      "Epoch 5700/10000 w1 0.677 w2 0.601 w3 0.741 b 0.014 loss 2.424613\n",
      "Epoch 5800/10000 w1 0.677 w2 0.601 w3 0.742 b 0.014 loss 2.423261\n",
      "Epoch 5900/10000 w1 0.677 w2 0.600 w3 0.743 b 0.014 loss 2.421929\n",
      "Epoch 6000/10000 w1 0.676 w2 0.599 w3 0.744 b 0.014 loss 2.420619\n",
      "Epoch 6100/10000 w1 0.676 w2 0.599 w3 0.745 b 0.014 loss 2.419294\n",
      "Epoch 6200/10000 w1 0.676 w2 0.598 w3 0.746 b 0.014 loss 2.418049\n",
      "Epoch 6300/10000 w1 0.676 w2 0.597 w3 0.747 b 0.015 loss 2.416759\n",
      "Epoch 6400/10000 w1 0.675 w2 0.597 w3 0.748 b 0.015 loss 2.415482\n",
      "Epoch 6500/10000 w1 0.675 w2 0.596 w3 0.748 b 0.015 loss 2.414237\n",
      "Epoch 6600/10000 w1 0.675 w2 0.596 w3 0.749 b 0.015 loss 2.413003\n",
      "Epoch 6700/10000 w1 0.674 w2 0.595 w3 0.750 b 0.015 loss 2.411770\n",
      "Epoch 6800/10000 w1 0.674 w2 0.594 w3 0.751 b 0.015 loss 2.410543\n",
      "Epoch 6900/10000 w1 0.674 w2 0.594 w3 0.752 b 0.015 loss 2.409352\n",
      "Epoch 7000/10000 w1 0.673 w2 0.593 w3 0.753 b 0.015 loss 2.408159\n",
      "Epoch 7100/10000 w1 0.673 w2 0.593 w3 0.754 b 0.015 loss 2.406968\n",
      "Epoch 7200/10000 w1 0.673 w2 0.592 w3 0.755 b 0.016 loss 2.405789\n",
      "Epoch 7300/10000 w1 0.672 w2 0.592 w3 0.755 b 0.016 loss 2.404649\n",
      "Epoch 7400/10000 w1 0.672 w2 0.591 w3 0.756 b 0.016 loss 2.403491\n",
      "Epoch 7500/10000 w1 0.672 w2 0.591 w3 0.757 b 0.016 loss 2.402358\n",
      "Epoch 7600/10000 w1 0.671 w2 0.590 w3 0.758 b 0.016 loss 2.401208\n",
      "Epoch 7700/10000 w1 0.671 w2 0.590 w3 0.759 b 0.016 loss 2.400069\n",
      "Epoch 7800/10000 w1 0.671 w2 0.589 w3 0.760 b 0.016 loss 2.398973\n",
      "Epoch 7900/10000 w1 0.670 w2 0.589 w3 0.760 b 0.016 loss 2.397860\n",
      "Epoch 8000/10000 w1 0.670 w2 0.588 w3 0.761 b 0.017 loss 2.396778\n",
      "Epoch 8100/10000 w1 0.670 w2 0.588 w3 0.762 b 0.017 loss 2.395677\n",
      "Epoch 8200/10000 w1 0.669 w2 0.587 w3 0.763 b 0.017 loss 2.394612\n",
      "Epoch 8300/10000 w1 0.669 w2 0.587 w3 0.764 b 0.017 loss 2.393550\n",
      "Epoch 8400/10000 w1 0.668 w2 0.586 w3 0.765 b 0.017 loss 2.392471\n",
      "Epoch 8500/10000 w1 0.668 w2 0.586 w3 0.765 b 0.017 loss 2.391407\n",
      "Epoch 8600/10000 w1 0.668 w2 0.585 w3 0.766 b 0.017 loss 2.390360\n",
      "Epoch 8700/10000 w1 0.667 w2 0.585 w3 0.767 b 0.017 loss 2.389329\n",
      "Epoch 8800/10000 w1 0.667 w2 0.584 w3 0.768 b 0.017 loss 2.388288\n",
      "Epoch 8900/10000 w1 0.667 w2 0.584 w3 0.769 b 0.018 loss 2.387273\n",
      "Epoch 9000/10000 w1 0.666 w2 0.583 w3 0.770 b 0.018 loss 2.386237\n",
      "Epoch 9100/10000 w1 0.666 w2 0.583 w3 0.770 b 0.018 loss 2.385236\n",
      "Epoch 9200/10000 w1 0.665 w2 0.582 w3 0.771 b 0.018 loss 2.384229\n",
      "Epoch 9300/10000 w1 0.665 w2 0.582 w3 0.772 b 0.018 loss 2.383246\n",
      "Epoch 9400/10000 w1 0.665 w2 0.582 w3 0.773 b 0.018 loss 2.382259\n",
      "Epoch 9500/10000 w1 0.664 w2 0.581 w3 0.774 b 0.018 loss 2.381262\n",
      "Epoch 9600/10000 w1 0.664 w2 0.581 w3 0.774 b 0.018 loss 2.380299\n",
      "Epoch 9700/10000 w1 0.664 w2 0.580 w3 0.775 b 0.019 loss 2.379318\n",
      "Epoch 9800/10000 w1 0.663 w2 0.580 w3 0.776 b 0.019 loss 2.378347\n",
      "Epoch 9900/10000 w1 0.663 w2 0.579 w3 0.777 b 0.019 loss 2.377397\n",
      "Epoch 10000/10000 w1 0.662 w2 0.579 w3 0.778 b 0.019 loss 2.376444\n"
     ]
    }
   ],
   "source": [
    "# 학습 몇번 진행할래 ?\n",
    "epoch_num = 10000\n",
    "for epoch in range(epoch_num + 1):\n",
    "\n",
    "    # 가설 xw + xw .... + b\n",
    "    hypothesis = x1_train * w1 + x2_train * w2 + x3_train * w3 + b\n",
    "\n",
    "    # loss\n",
    "    loss = torch.mean((hypothesis - y_train) ** 2)\n",
    "\n",
    "    # loss H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(\n",
    "            \"Epoch {:4d}/{} w1 {:.3f} w2 {:.3f} w3 {:.3f} b {:.3f} loss {:.6f}\".format(\n",
    "                epoch, epoch_num, w1.item(), w2.item(), w3.item(), b.item(), loss.item()\n",
    "            ))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Linear\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 생성\n",
    "x_train = torch.FloatTensor([[73, 80, 75],\n",
    "                            [93, 88, 93],\n",
    "                            [89, 91, 90],\n",
    "                            [96, 98, 100],\n",
    "                            [73, 66, 70]\n",
    "                             ])\n",
    "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class 생성\n",
    "class MultivariateLinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(3, 1)  # input 3 output 1\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 정의\n",
    "model = MultivariateLinearRegressionModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :    0/2000 loss : 23378.402344\n",
      "Epoch :  100/2000 loss : 1.603486\n",
      "Epoch :  200/2000 loss : 1.529637\n",
      "Epoch :  300/2000 loss : 1.459686\n",
      "Epoch :  400/2000 loss : 1.393411\n",
      "Epoch :  500/2000 loss : 1.330631\n",
      "Epoch :  600/2000 loss : 1.271155\n",
      "Epoch :  700/2000 loss : 1.214824\n",
      "Epoch :  800/2000 loss : 1.161444\n",
      "Epoch :  900/2000 loss : 1.110880\n",
      "Epoch : 1000/2000 loss : 1.062996\n",
      "Epoch : 1100/2000 loss : 1.017612\n",
      "Epoch : 1200/2000 loss : 0.974614\n",
      "Epoch : 1300/2000 loss : 0.933897\n",
      "Epoch : 1400/2000 loss : 0.895324\n",
      "Epoch : 1500/2000 loss : 0.858785\n",
      "Epoch : 1600/2000 loss : 0.824152\n",
      "Epoch : 1700/2000 loss : 0.791347\n",
      "Epoch : 1800/2000 loss : 0.760271\n",
      "Epoch : 1900/2000 loss : 0.730823\n",
      "Epoch : 2000/2000 loss : 0.702931\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "epochs_num = 2000\n",
    "for epoch in range(epochs_num + 1):\n",
    "\n",
    "    # model\n",
    "    prediction = model(x_train)\n",
    "\n",
    "    # loss\n",
    "    loss = F.mse_loss(prediction, y_train)\n",
    "\n",
    "    # loss 개선\n",
    "    optimizer.zero_grad()  # 기울기를 0으로 초기화\n",
    "    loss.backward()       # loss 함수를 미분하여 기울기 계산\n",
    "    optimizer.step()      # w , b 를 업데이트\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(\"Epoch : {:4d}/{} loss : {:.6f}\".format(\n",
    "            epoch, epochs_num, loss.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 후 입력이 tensor([[73., 82., 72.]]) 에측값 : tensor([[153.3170]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "new_var = torch.FloatTensor([[73, 82, 72]])\n",
    "pred_y = model(new_var)\n",
    "print(f\"훈련 후 입력이 {new_var} 에측값 : {pred_y}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 다른 epoch값으로 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 생성\n",
    "x_train = torch.FloatTensor([[73, 80, 75],\n",
    "                            [93, 88, 93],\n",
    "                            [89, 91, 90],\n",
    "                            [96, 98, 100],\n",
    "                            [73, 66, 70]\n",
    "                             ])\n",
    "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorDataset 입력으로 사용하고 dataset 지정합니다.\n",
    "dataset = TensorDataset(x_train, y_train)\n",
    "\n",
    "# dataloader\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "# model 설계\n",
    "model = nn.Linear(3, 1)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/300 batch 1/3 loss : 14516.478516\n",
      "Epoch    0/300 batch 2/3 loss : 3546.791016\n",
      "Epoch    0/300 batch 3/3 loss : 639.297852\n",
      "Epoch   10/300 batch 1/3 loss : 0.012521\n",
      "Epoch   10/300 batch 2/3 loss : 0.261185\n",
      "Epoch   10/300 batch 3/3 loss : 3.439424\n",
      "Epoch   20/300 batch 1/3 loss : 1.121956\n",
      "Epoch   20/300 batch 2/3 loss : 0.731954\n",
      "Epoch   20/300 batch 3/3 loss : 0.043643\n",
      "Epoch   30/300 batch 1/3 loss : 0.363889\n",
      "Epoch   30/300 batch 2/3 loss : 0.006787\n",
      "Epoch   30/300 batch 3/3 loss : 3.057642\n",
      "Epoch   40/300 batch 1/3 loss : 1.658781\n",
      "Epoch   40/300 batch 2/3 loss : 0.043797\n",
      "Epoch   40/300 batch 3/3 loss : 0.619878\n",
      "Epoch   50/300 batch 1/3 loss : 1.530616\n",
      "Epoch   50/300 batch 2/3 loss : 0.054902\n",
      "Epoch   50/300 batch 3/3 loss : 0.086512\n",
      "Epoch   60/300 batch 1/3 loss : 0.108856\n",
      "Epoch   60/300 batch 2/3 loss : 0.079088\n",
      "Epoch   60/300 batch 3/3 loss : 3.271232\n",
      "Epoch   70/300 batch 1/3 loss : 1.564638\n",
      "Epoch   70/300 batch 2/3 loss : 0.091355\n",
      "Epoch   70/300 batch 3/3 loss : 0.010399\n",
      "Epoch   80/300 batch 1/3 loss : 1.529838\n",
      "Epoch   80/300 batch 2/3 loss : 0.171671\n",
      "Epoch   80/300 batch 3/3 loss : 0.567110\n",
      "Epoch   90/300 batch 1/3 loss : 0.013888\n",
      "Epoch   90/300 batch 2/3 loss : 1.361205\n",
      "Epoch   90/300 batch 3/3 loss : 0.972843\n",
      "Epoch  100/300 batch 1/3 loss : 0.024635\n",
      "Epoch  100/300 batch 2/3 loss : 1.411437\n",
      "Epoch  100/300 batch 3/3 loss : 1.044272\n",
      "Epoch  110/300 batch 1/3 loss : 1.114485\n",
      "Epoch  110/300 batch 2/3 loss : 0.648803\n",
      "Epoch  110/300 batch 3/3 loss : 0.039470\n",
      "Epoch  120/300 batch 1/3 loss : 0.096140\n",
      "Epoch  120/300 batch 2/3 loss : 0.252014\n",
      "Epoch  120/300 batch 3/3 loss : 3.004987\n",
      "Epoch  130/300 batch 1/3 loss : 1.532440\n",
      "Epoch  130/300 batch 2/3 loss : 0.035911\n",
      "Epoch  130/300 batch 3/3 loss : 0.050107\n",
      "Epoch  140/300 batch 1/3 loss : 0.141976\n",
      "Epoch  140/300 batch 2/3 loss : 0.278688\n",
      "Epoch  140/300 batch 3/3 loss : 2.904170\n",
      "Epoch  150/300 batch 1/3 loss : 1.284018\n",
      "Epoch  150/300 batch 2/3 loss : 0.689716\n",
      "Epoch  150/300 batch 3/3 loss : 0.006435\n",
      "Epoch  160/300 batch 1/3 loss : 1.118297\n",
      "Epoch  160/300 batch 2/3 loss : 1.189957\n",
      "Epoch  160/300 batch 3/3 loss : 0.285038\n",
      "Epoch  170/300 batch 1/3 loss : 1.500112\n",
      "Epoch  170/300 batch 2/3 loss : 0.037206\n",
      "Epoch  170/300 batch 3/3 loss : 0.043623\n",
      "Epoch  180/300 batch 1/3 loss : 1.566508\n",
      "Epoch  180/300 batch 2/3 loss : 0.371175\n",
      "Epoch  180/300 batch 3/3 loss : 0.006063\n",
      "Epoch  190/300 batch 1/3 loss : 0.100940\n",
      "Epoch  190/300 batch 2/3 loss : 1.631142\n",
      "Epoch  190/300 batch 3/3 loss : 0.024500\n",
      "Epoch  200/300 batch 1/3 loss : 0.271526\n",
      "Epoch  200/300 batch 2/3 loss : 0.000103\n",
      "Epoch  200/300 batch 3/3 loss : 2.819813\n",
      "Epoch  210/300 batch 1/3 loss : 1.386603\n",
      "Epoch  210/300 batch 2/3 loss : 0.131801\n",
      "Epoch  210/300 batch 3/3 loss : 0.014553\n",
      "Epoch  220/300 batch 1/3 loss : 1.555426\n",
      "Epoch  220/300 batch 2/3 loss : 0.389689\n",
      "Epoch  220/300 batch 3/3 loss : 0.075907\n",
      "Epoch  230/300 batch 1/3 loss : 0.366927\n",
      "Epoch  230/300 batch 2/3 loss : 1.430454\n",
      "Epoch  230/300 batch 3/3 loss : 0.000203\n",
      "Epoch  240/300 batch 1/3 loss : 0.021935\n",
      "Epoch  240/300 batch 2/3 loss : 0.281646\n",
      "Epoch  240/300 batch 3/3 loss : 2.951270\n",
      "Epoch  250/300 batch 1/3 loss : 0.010266\n",
      "Epoch  250/300 batch 2/3 loss : 0.209391\n",
      "Epoch  250/300 batch 3/3 loss : 2.908228\n",
      "Epoch  260/300 batch 1/3 loss : 0.070484\n",
      "Epoch  260/300 batch 2/3 loss : 0.138326\n",
      "Epoch  260/300 batch 3/3 loss : 3.139725\n",
      "Epoch  270/300 batch 1/3 loss : 1.368075\n",
      "Epoch  270/300 batch 2/3 loss : 0.065556\n",
      "Epoch  270/300 batch 3/3 loss : 0.677075\n",
      "Epoch  280/300 batch 1/3 loss : 0.282354\n",
      "Epoch  280/300 batch 2/3 loss : 1.332343\n",
      "Epoch  280/300 batch 3/3 loss : 0.140122\n",
      "Epoch  290/300 batch 1/3 loss : 0.007886\n",
      "Epoch  290/300 batch 2/3 loss : 1.514362\n",
      "Epoch  290/300 batch 3/3 loss : 0.024433\n",
      "Epoch  300/300 batch 1/3 loss : 0.185125\n",
      "Epoch  300/300 batch 2/3 loss : 0.902945\n",
      "Epoch  300/300 batch 3/3 loss : 1.392815\n"
     ]
    }
   ],
   "source": [
    "# train loop\n",
    "epoch_number = 300\n",
    "for epoch in range(epoch_number + 1):\n",
    "    for batch_idx, sample in enumerate(dataloader):\n",
    "        x_train, y_train = sample\n",
    "\n",
    "        prediction = model(x_train)\n",
    "\n",
    "        # loss - pytorch에서 제공하는 평균 제곱 오차 방법 사용\n",
    "        loss = F.mse_loss(prediction, y_train)\n",
    "\n",
    "        # loss H(x) 계산\n",
    "        optimizer.zero_grad() # 기울기 0으로 초기화\n",
    "        loss.backward() # loss 함수를 미분해서 기울기 계산\n",
    "        optimizer.step() # w, b를 업데이트\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(\"Epoch {:4d}/{} batch {}/{} loss : {:.6f}\".format(\n",
    "                epoch, epoch_number, batch_idx+1, len(dataloader), loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150.33485412597656\n"
     ]
    }
   ],
   "source": [
    "test_val = torch.FloatTensor([[73, 80, 75]])\n",
    "\n",
    "pred_y = model(test_val)\n",
    "print(pred_y.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e89c93e4c07d4ac8f065cea982a638287e1c61026788fcbbad7e0263e2130583"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
