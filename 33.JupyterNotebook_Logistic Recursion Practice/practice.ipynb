{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pytorch의 nn.Linear와 nn.Sigmoid로 로지스틱 회귀 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data -> tensor\n",
    "x_data = [[1,2], [2,3], [3,1], [4,3], [5,3], [6,2]]\n",
    "y_data = [[0], [0], [0], [1], [1], [1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor로 변환\n",
    "x_train = torch.FloatTensor(x_data)\n",
    "y_train = torch.FloatTensor(y_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.Sequential()\n",
    "- nn.Sequential()은 수식 및 함수들을 연결해주는 역할을 함\n",
    "\n",
    "- 신경망 층을 쌓을 때 사용\n",
    "\n",
    "- x 입력층은 2 -> y 출력층은 1\n",
    "    - ex) [1,2] --> [0]\n",
    "\n",
    "- 출력은 sigmoid function을 거침"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8319],\n",
      "        [0.9042],\n",
      "        [0.8936],\n",
      "        [0.9538],\n",
      "        [0.9683],\n",
      "        [0.9722]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = nn.Sequential( \n",
    "    nn.Linear(2,1), \n",
    "    nn.Sigmoid() \n",
    ")\n",
    "print(model(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "epoch_num = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :    0/1000, Loss 1.079477, Acc : 50.00%\n",
      "Epoch :   10/1000, Loss 0.655287, Acc : 50.00%\n",
      "Epoch :   20/1000, Loss 0.601358, Acc : 83.33%\n",
      "Epoch :   30/1000, Loss 0.564895, Acc : 83.33%\n",
      "Epoch :   40/1000, Loss 0.538716, Acc : 83.33%\n",
      "Epoch :   50/1000, Loss 0.518769, Acc : 83.33%\n",
      "Epoch :   60/1000, Loss 0.502737, Acc : 83.33%\n",
      "Epoch :   70/1000, Loss 0.489257, Acc : 83.33%\n",
      "Epoch :   80/1000, Loss 0.477502, Acc : 83.33%\n",
      "Epoch :   90/1000, Loss 0.466956, Acc : 83.33%\n",
      "Epoch :  100/1000, Loss 0.457287, Acc : 83.33%\n",
      "Epoch :  110/1000, Loss 0.448277, Acc : 83.33%\n",
      "Epoch :  120/1000, Loss 0.439779, Acc : 83.33%\n",
      "Epoch :  130/1000, Loss 0.431693, Acc : 83.33%\n",
      "Epoch :  140/1000, Loss 0.423947, Acc : 83.33%\n",
      "Epoch :  150/1000, Loss 0.416493, Acc : 83.33%\n",
      "Epoch :  160/1000, Loss 0.409294, Acc : 83.33%\n",
      "Epoch :  170/1000, Loss 0.402323, Acc : 83.33%\n",
      "Epoch :  180/1000, Loss 0.395561, Acc : 83.33%\n",
      "Epoch :  190/1000, Loss 0.388991, Acc : 83.33%\n",
      "Epoch :  200/1000, Loss 0.382602, Acc : 83.33%\n",
      "Epoch :  210/1000, Loss 0.376385, Acc : 83.33%\n",
      "Epoch :  220/1000, Loss 0.370331, Acc : 83.33%\n",
      "Epoch :  230/1000, Loss 0.364433, Acc : 83.33%\n",
      "Epoch :  240/1000, Loss 0.358686, Acc : 83.33%\n",
      "Epoch :  250/1000, Loss 0.353084, Acc : 83.33%\n",
      "Epoch :  260/1000, Loss 0.347623, Acc : 83.33%\n",
      "Epoch :  270/1000, Loss 0.342298, Acc : 83.33%\n",
      "Epoch :  280/1000, Loss 0.337105, Acc : 83.33%\n",
      "Epoch :  290/1000, Loss 0.332040, Acc : 83.33%\n",
      "Epoch :  300/1000, Loss 0.327100, Acc : 83.33%\n",
      "Epoch :  310/1000, Loss 0.322281, Acc : 83.33%\n",
      "Epoch :  320/1000, Loss 0.317580, Acc : 83.33%\n",
      "Epoch :  330/1000, Loss 0.312993, Acc : 83.33%\n",
      "Epoch :  340/1000, Loss 0.308517, Acc : 83.33%\n",
      "Epoch :  350/1000, Loss 0.304150, Acc : 83.33%\n",
      "Epoch :  360/1000, Loss 0.299887, Acc : 83.33%\n",
      "Epoch :  370/1000, Loss 0.295726, Acc : 83.33%\n",
      "Epoch :  380/1000, Loss 0.291664, Acc : 83.33%\n",
      "Epoch :  390/1000, Loss 0.287698, Acc : 83.33%\n",
      "Epoch :  400/1000, Loss 0.283826, Acc : 83.33%\n",
      "Epoch :  410/1000, Loss 0.280045, Acc : 83.33%\n",
      "Epoch :  420/1000, Loss 0.276352, Acc : 83.33%\n",
      "Epoch :  430/1000, Loss 0.272744, Acc : 83.33%\n",
      "Epoch :  440/1000, Loss 0.269220, Acc : 83.33%\n",
      "Epoch :  450/1000, Loss 0.265777, Acc : 83.33%\n",
      "Epoch :  460/1000, Loss 0.262412, Acc : 83.33%\n",
      "Epoch :  470/1000, Loss 0.259124, Acc : 83.33%\n",
      "Epoch :  480/1000, Loss 0.255909, Acc : 83.33%\n",
      "Epoch :  490/1000, Loss 0.252767, Acc : 100.00%\n",
      "Epoch :  500/1000, Loss 0.249694, Acc : 100.00%\n",
      "Epoch :  510/1000, Loss 0.246690, Acc : 100.00%\n",
      "Epoch :  520/1000, Loss 0.243751, Acc : 100.00%\n",
      "Epoch :  530/1000, Loss 0.240877, Acc : 100.00%\n",
      "Epoch :  540/1000, Loss 0.238064, Acc : 100.00%\n",
      "Epoch :  550/1000, Loss 0.235313, Acc : 100.00%\n",
      "Epoch :  560/1000, Loss 0.232619, Acc : 100.00%\n",
      "Epoch :  570/1000, Loss 0.229984, Acc : 100.00%\n",
      "Epoch :  580/1000, Loss 0.227403, Acc : 100.00%\n",
      "Epoch :  590/1000, Loss 0.224877, Acc : 100.00%\n",
      "Epoch :  600/1000, Loss 0.222403, Acc : 100.00%\n",
      "Epoch :  610/1000, Loss 0.219980, Acc : 100.00%\n",
      "Epoch :  620/1000, Loss 0.217606, Acc : 100.00%\n",
      "Epoch :  630/1000, Loss 0.215281, Acc : 100.00%\n",
      "Epoch :  640/1000, Loss 0.213003, Acc : 100.00%\n",
      "Epoch :  650/1000, Loss 0.210770, Acc : 100.00%\n",
      "Epoch :  660/1000, Loss 0.208582, Acc : 100.00%\n",
      "Epoch :  670/1000, Loss 0.206437, Acc : 100.00%\n",
      "Epoch :  680/1000, Loss 0.204334, Acc : 100.00%\n",
      "Epoch :  690/1000, Loss 0.202271, Acc : 100.00%\n",
      "Epoch :  700/1000, Loss 0.200249, Acc : 100.00%\n",
      "Epoch :  710/1000, Loss 0.198265, Acc : 100.00%\n",
      "Epoch :  720/1000, Loss 0.196319, Acc : 100.00%\n",
      "Epoch :  730/1000, Loss 0.194410, Acc : 100.00%\n",
      "Epoch :  740/1000, Loss 0.192536, Acc : 100.00%\n",
      "Epoch :  750/1000, Loss 0.190697, Acc : 100.00%\n",
      "Epoch :  760/1000, Loss 0.188892, Acc : 100.00%\n",
      "Epoch :  770/1000, Loss 0.187121, Acc : 100.00%\n",
      "Epoch :  780/1000, Loss 0.185381, Acc : 100.00%\n",
      "Epoch :  790/1000, Loss 0.183673, Acc : 100.00%\n",
      "Epoch :  800/1000, Loss 0.181995, Acc : 100.00%\n",
      "Epoch :  810/1000, Loss 0.180347, Acc : 100.00%\n",
      "Epoch :  820/1000, Loss 0.178729, Acc : 100.00%\n",
      "Epoch :  830/1000, Loss 0.177138, Acc : 100.00%\n",
      "Epoch :  840/1000, Loss 0.175575, Acc : 100.00%\n",
      "Epoch :  850/1000, Loss 0.174040, Acc : 100.00%\n",
      "Epoch :  860/1000, Loss 0.172530, Acc : 100.00%\n",
      "Epoch :  870/1000, Loss 0.171047, Acc : 100.00%\n",
      "Epoch :  880/1000, Loss 0.169588, Acc : 100.00%\n",
      "Epoch :  890/1000, Loss 0.168154, Acc : 100.00%\n",
      "Epoch :  900/1000, Loss 0.166743, Acc : 100.00%\n",
      "Epoch :  910/1000, Loss 0.165356, Acc : 100.00%\n",
      "Epoch :  920/1000, Loss 0.163992, Acc : 100.00%\n",
      "Epoch :  930/1000, Loss 0.162650, Acc : 100.00%\n",
      "Epoch :  940/1000, Loss 0.161329, Acc : 100.00%\n",
      "Epoch :  950/1000, Loss 0.160030, Acc : 100.00%\n",
      "Epoch :  960/1000, Loss 0.158752, Acc : 100.00%\n",
      "Epoch :  970/1000, Loss 0.157493, Acc : 100.00%\n",
      "Epoch :  980/1000, Loss 0.156255, Acc : 100.00%\n",
      "Epoch :  990/1000, Loss 0.155036, Acc : 100.00%\n",
      "Epoch : 1000/1000, Loss 0.153836, Acc : 100.00%\n",
      "tensor([[0.0325],\n",
      "        [0.1612],\n",
      "        [0.3134],\n",
      "        [0.7775],\n",
      "        [0.9371],\n",
      "        [0.9793]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epoch_num + 1) :\n",
    "    output = model(x_train)\n",
    "\n",
    "    # loss\n",
    "    loss = F.binary_cross_entropy(output, y_train)\n",
    "\n",
    "    # loss H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0 :\n",
    "        prediction = output >= torch.FloatTensor([0.5]) # 예측값이 0.5가 넘으면 True로 간주\n",
    "        correct_prediction = prediction.float() == y_train # 실제값과 일치하는 경우만 True\n",
    "        acc = correct_prediction.sum().item() / len(correct_prediction) # 정확도 계산\n",
    "        print(\"Epoch : {:4d}/{}, Loss {:.6f}, Acc : {:.2f}%\".format(\n",
    "            epoch, epoch_num, loss.item(), acc * 100\n",
    "        ))\n",
    "\n",
    "print(model(x_train))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 클래스로 pytorch 모델 구현 : 로지스틱 회귀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. x data, y data\n",
    "x_data = [[1,2], [2,3], [3,1], [4,3], [5,3], [6,2]]\n",
    "y_data = [[0], [0], [0], [1], [1], [1]]\n",
    "\n",
    "# 2. x data, y data --> tensor 변환\n",
    "x_train = torch.FloatTensor(x_data)\n",
    "y_train = torch.FloatTensor(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryClassifier(nn.Module) :\n",
    "    def __init__(self) :\n",
    "        super().__init__()\n",
    "        # input dimension = 2, output dimension = 1\n",
    "        self.linear = nn.Linear(2,1)\n",
    "        # sigmoid 함수를 거쳐서 출력 \n",
    "        self.sigmoid = nn.Sigmoid() \n",
    "    \n",
    "    def forward(self, x) :\n",
    "        return self.sigmoid(self.linear(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BinaryClassifier(\n",
      "  (linear): Linear(in_features=2, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 3. 모델 호출\n",
    "model = BinaryClassifier()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. optimizer 설정\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "\n",
    "# 5. 얼마만큼 반복할 거냐?\n",
    "epoch_num = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :    0/1000, Loss 0.685524, Acc : 50.00%\n",
      "Epoch :   10/1000, Loss 0.427183, Acc : 83.33%\n",
      "Epoch :   20/1000, Loss 0.366219, Acc : 83.33%\n",
      "Epoch :   30/1000, Loss 0.311767, Acc : 83.33%\n",
      "Epoch :   40/1000, Loss 0.261134, Acc : 83.33%\n",
      "Epoch :   50/1000, Loss 0.227228, Acc : 100.00%\n",
      "Epoch :   60/1000, Loss 0.201937, Acc : 100.00%\n",
      "Epoch :   70/1000, Loss 0.181270, Acc : 100.00%\n",
      "Epoch :   80/1000, Loss 0.164524, Acc : 100.00%\n",
      "Epoch :   90/1000, Loss 0.150795, Acc : 100.00%\n",
      "Epoch :  100/1000, Loss 0.139271, Acc : 100.00%\n",
      "Epoch :  110/1000, Loss 0.129462, Acc : 100.00%\n",
      "Epoch :  120/1000, Loss 0.121015, Acc : 100.00%\n",
      "Epoch :  130/1000, Loss 0.113658, Acc : 100.00%\n",
      "Epoch :  140/1000, Loss 0.107190, Acc : 100.00%\n",
      "Epoch :  150/1000, Loss 0.101455, Acc : 100.00%\n",
      "Epoch :  160/1000, Loss 0.096334, Acc : 100.00%\n",
      "Epoch :  170/1000, Loss 0.091730, Acc : 100.00%\n",
      "Epoch :  180/1000, Loss 0.087567, Acc : 100.00%\n",
      "Epoch :  190/1000, Loss 0.083784, Acc : 100.00%\n",
      "Epoch :  200/1000, Loss 0.080329, Acc : 100.00%\n",
      "Epoch :  210/1000, Loss 0.077161, Acc : 100.00%\n",
      "Epoch :  220/1000, Loss 0.074245, Acc : 100.00%\n",
      "Epoch :  230/1000, Loss 0.071551, Acc : 100.00%\n",
      "Epoch :  240/1000, Loss 0.069054, Acc : 100.00%\n",
      "Epoch :  250/1000, Loss 0.066733, Acc : 100.00%\n",
      "Epoch :  260/1000, Loss 0.064569, Acc : 100.00%\n",
      "Epoch :  270/1000, Loss 0.062547, Acc : 100.00%\n",
      "Epoch :  280/1000, Loss 0.060652, Acc : 100.00%\n",
      "Epoch :  290/1000, Loss 0.058873, Acc : 100.00%\n",
      "Epoch :  300/1000, Loss 0.057200, Acc : 100.00%\n",
      "Epoch :  310/1000, Loss 0.055622, Acc : 100.00%\n",
      "Epoch :  320/1000, Loss 0.054133, Acc : 100.00%\n",
      "Epoch :  330/1000, Loss 0.052724, Acc : 100.00%\n",
      "Epoch :  340/1000, Loss 0.051388, Acc : 100.00%\n",
      "Epoch :  350/1000, Loss 0.050121, Acc : 100.00%\n",
      "Epoch :  360/1000, Loss 0.048917, Acc : 100.00%\n",
      "Epoch :  370/1000, Loss 0.047772, Acc : 100.00%\n",
      "Epoch :  380/1000, Loss 0.046680, Acc : 100.00%\n",
      "Epoch :  390/1000, Loss 0.045639, Acc : 100.00%\n",
      "Epoch :  400/1000, Loss 0.044644, Acc : 100.00%\n",
      "Epoch :  410/1000, Loss 0.043693, Acc : 100.00%\n",
      "Epoch :  420/1000, Loss 0.042783, Acc : 100.00%\n",
      "Epoch :  430/1000, Loss 0.041912, Acc : 100.00%\n",
      "Epoch :  440/1000, Loss 0.041076, Acc : 100.00%\n",
      "Epoch :  450/1000, Loss 0.040273, Acc : 100.00%\n",
      "Epoch :  460/1000, Loss 0.039502, Acc : 100.00%\n",
      "Epoch :  470/1000, Loss 0.038761, Acc : 100.00%\n",
      "Epoch :  480/1000, Loss 0.038048, Acc : 100.00%\n",
      "Epoch :  490/1000, Loss 0.037361, Acc : 100.00%\n",
      "Epoch :  500/1000, Loss 0.036699, Acc : 100.00%\n",
      "Epoch :  510/1000, Loss 0.036061, Acc : 100.00%\n",
      "Epoch :  520/1000, Loss 0.035445, Acc : 100.00%\n",
      "Epoch :  530/1000, Loss 0.034850, Acc : 100.00%\n",
      "Epoch :  540/1000, Loss 0.034276, Acc : 100.00%\n",
      "Epoch :  550/1000, Loss 0.033720, Acc : 100.00%\n",
      "Epoch :  560/1000, Loss 0.033183, Acc : 100.00%\n",
      "Epoch :  570/1000, Loss 0.032662, Acc : 100.00%\n",
      "Epoch :  580/1000, Loss 0.032159, Acc : 100.00%\n",
      "Epoch :  590/1000, Loss 0.031670, Acc : 100.00%\n",
      "Epoch :  600/1000, Loss 0.031197, Acc : 100.00%\n",
      "Epoch :  610/1000, Loss 0.030738, Acc : 100.00%\n",
      "Epoch :  620/1000, Loss 0.030292, Acc : 100.00%\n",
      "Epoch :  630/1000, Loss 0.029860, Acc : 100.00%\n",
      "Epoch :  640/1000, Loss 0.029439, Acc : 100.00%\n",
      "Epoch :  650/1000, Loss 0.029031, Acc : 100.00%\n",
      "Epoch :  660/1000, Loss 0.028634, Acc : 100.00%\n",
      "Epoch :  670/1000, Loss 0.028248, Acc : 100.00%\n",
      "Epoch :  680/1000, Loss 0.027873, Acc : 100.00%\n",
      "Epoch :  690/1000, Loss 0.027507, Acc : 100.00%\n",
      "Epoch :  700/1000, Loss 0.027151, Acc : 100.00%\n",
      "Epoch :  710/1000, Loss 0.026804, Acc : 100.00%\n",
      "Epoch :  720/1000, Loss 0.026467, Acc : 100.00%\n",
      "Epoch :  730/1000, Loss 0.026137, Acc : 100.00%\n",
      "Epoch :  740/1000, Loss 0.025816, Acc : 100.00%\n",
      "Epoch :  750/1000, Loss 0.025503, Acc : 100.00%\n",
      "Epoch :  760/1000, Loss 0.025198, Acc : 100.00%\n",
      "Epoch :  770/1000, Loss 0.024899, Acc : 100.00%\n",
      "Epoch :  780/1000, Loss 0.024608, Acc : 100.00%\n",
      "Epoch :  790/1000, Loss 0.024324, Acc : 100.00%\n",
      "Epoch :  800/1000, Loss 0.024046, Acc : 100.00%\n",
      "Epoch :  810/1000, Loss 0.023775, Acc : 100.00%\n",
      "Epoch :  820/1000, Loss 0.023510, Acc : 100.00%\n",
      "Epoch :  830/1000, Loss 0.023251, Acc : 100.00%\n",
      "Epoch :  840/1000, Loss 0.022997, Acc : 100.00%\n",
      "Epoch :  850/1000, Loss 0.022749, Acc : 100.00%\n",
      "Epoch :  860/1000, Loss 0.022507, Acc : 100.00%\n",
      "Epoch :  870/1000, Loss 0.022269, Acc : 100.00%\n",
      "Epoch :  880/1000, Loss 0.022037, Acc : 100.00%\n",
      "Epoch :  890/1000, Loss 0.021809, Acc : 100.00%\n",
      "Epoch :  900/1000, Loss 0.021586, Acc : 100.00%\n",
      "Epoch :  910/1000, Loss 0.021368, Acc : 100.00%\n",
      "Epoch :  920/1000, Loss 0.021154, Acc : 100.00%\n",
      "Epoch :  930/1000, Loss 0.020945, Acc : 100.00%\n",
      "Epoch :  940/1000, Loss 0.020739, Acc : 100.00%\n",
      "Epoch :  950/1000, Loss 0.020538, Acc : 100.00%\n",
      "Epoch :  960/1000, Loss 0.020341, Acc : 100.00%\n",
      "Epoch :  970/1000, Loss 0.020147, Acc : 100.00%\n",
      "Epoch :  980/1000, Loss 0.019957, Acc : 100.00%\n",
      "Epoch :  990/1000, Loss 0.019771, Acc : 100.00%\n",
      "Epoch : 1000/1000, Loss 0.019588, Acc : 100.00%\n",
      "tensor([[2.6748e-04],\n",
      "        [3.1212e-02],\n",
      "        [3.8452e-02],\n",
      "        [9.5676e-01],\n",
      "        [9.9828e-01],\n",
      "        [9.9970e-01]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 6. 학습\n",
    "for epoch in range(epoch_num + 1) :\n",
    "    output = model(x_train)\n",
    "\n",
    "    # loss\n",
    "    loss = F.binary_cross_entropy(output, y_train)\n",
    "\n",
    "    # loss H(x) 개선\n",
    "    optimizer.zero_grad() # gradient를 None으로 설정(=초기화)\n",
    "    loss.backward()\n",
    "    optimizer.step() # 단일 optimization step을 수행하고 pararmeter를 업데이트함\n",
    "\n",
    "    if epoch % 10 == 0 :\n",
    "        prediction = output >= torch.FloatTensor([0.5])\n",
    "        correct_prediction = prediction.float() == y_train\n",
    "        acc = correct_prediction.sum().item() / len(correct_prediction)\n",
    "        print(\"Epoch : {:4d}/{}, Loss {:.6f}, Acc : {:.2f}%\".format(\n",
    "            epoch, epoch_num, loss.item(), acc * 100\n",
    "        ))\n",
    "        \n",
    "print(model(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e89c93e4c07d4ac8f065cea982a638287e1c61026788fcbbad7e0263e2130583"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
