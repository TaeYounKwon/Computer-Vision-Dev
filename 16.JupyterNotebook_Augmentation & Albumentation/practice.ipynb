{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation 데이터 확장\n",
    "\n",
    "- 한장의 사진을 여러 각도, 여러 색깔, 다르게 edit해서 컴퓨터 비전의 성능을 향상시키는 방법\n",
    "\n",
    "- 보통 Training단계에서 많이 쓰이지만, 상황에 따라 Test 단계에서도 사용이 가능, 이를 Test Time Augmentation(TTA)라고 함.\n",
    "    - 한장의 Test Image를 여러장으로 증강시켜 Interence를 시킨 뒤, 나온 output을 ensemble하는 방식이며,\n",
    "\n",
    "    - kaggle과 같은 챌린지에서 많이 사용됨\n",
    "\n",
    "- 간단히 말해 Do more with Less Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "import albumentations\n",
    "from albumentations.pytorch import ToTensorV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기존 torchvision Data pipeline\n",
    "# 1. dataset class --> image loader --> transforms\n",
    "class CatDataset(Dataset): # basic of custom dataset\n",
    "    def __init__(self, file_paths, transform=None):\n",
    "        self.file_paths = file_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        file_path = self.file_paths[index]\n",
    "\n",
    "        # 원래라면 image, label을 return 해야함(이미지와 정답지)\n",
    "\n",
    "        # Read an image with PIL\n",
    "        image = Image.open(file_path).convert(\"RGB\")\n",
    "\n",
    "        # transform time check(시간 얼마나 걸리나 재보자)\n",
    "        start_tiem = time.time() # 시작 시간\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        end_tiem = (time.time() - start_tiem)\n",
    "\n",
    "        return image, end_tiem\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 신규 albumentations Data pipeline\n",
    "class alb_cat_dataset(Dataset):\n",
    "    def __init__(self, file_paths, transform=None):\n",
    "        self.file_paths = file_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        file_path = self.file_paths[index]\n",
    "\n",
    "        # read an image with opencv\n",
    "        image = cv2.imread(file_path)\n",
    "\n",
    "        # BGR --> RGB로 변경\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        self.st_time = time.time()\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image=image)[\"image\"]\n",
    "            total_time = (time.time() - self.st_time)\n",
    "\n",
    "        return image, total_time\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation transforms\n",
    "# train용을 따로 만들어서 해야함\n",
    "# train_transform = transforms.Compose([\n",
    "#     ㅁㅁㅁㅁ\n",
    "# ])\n",
    "# val용\n",
    "# val_transform = transforms.Compose([\n",
    "#     ㅁㅁㅁㅁ(train과 동일하게 작성. 단, 랜덤 요소 제외. test는 어차피 고정값이라 val용 가져다 쓰면 됨.)\n",
    "# ])\n",
    "# 어떤 기능을 하는지 각각 캡처떠서 제출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torchvision_transform = transforms.Compose([\n",
    "    transforms.Pad(padding=100),\n",
    "    transforms.Resize((256,256)),\n",
    "    transforms.CenterCrop(size=(30)), # 사용을 거의 하지 않는다. 학습에 영 도움이 안 되는 crop이 됨. 만약, 이미지 상 다 동일한 위치면 사용해도 됨.\n",
    "    transforms.Grayscale(), # 특수한 경우에만 사용\n",
    "    transforms.ColorJitter(brightness=0.5, hue=0.2, contrast=0.3),\n",
    "    transforms.GaussianBlur(kernel_size=(3,9), sigma=(0.1, 5)),\n",
    "    transforms.RandomPerspective(distortion_scale=0.7, p=1.0),\n",
    "    transforms.RandomRotation(degrees=(0,100)),\n",
    "    transforms.RandomAffine(degrees=(30,60), translate=(0.1, 0.3), scale=(0.5,0.7)),\n",
    "    # transforms.ElasticTransform(alpha=255.0), # 버전 차이 --> 상위 버전에서 사용 가능\n",
    "    transforms.RandomEqualize(p=1),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.AutoAugment(), # 자동으로 아무거나 augmentation을 먹여줌. 뭐가 들어갈지 모름;;\n",
    "    \n",
    "    # # 비교용 transforms 설정\n",
    "    # transforms.Resize((256,256)),\n",
    "    # transforms.RandomCrop(224),\n",
    "    # transforms.RandomHorizontalFlip(),\n",
    "    # transforms.RandomVerticalFlip(),\n",
    "    transforms.ToTensor() # augmentation 처리된 결과(=이미지)를 tensor화 해줘!\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "albumentations_transform = albumentations.Compose([\n",
    "    albumentations.Resize(256, 256),\n",
    "    albumentations.RandomCrop(224, 224),\n",
    "    albumentations.HorizontalFlip(),\n",
    "    albumentations.VerticalFlip(),\n",
    "    # albumentations.pytorch.transforms.ToTensor(),  --> 이 버전에선 안 됨\n",
    "    ToTensorV2()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "albumentations_transform_oneof = albumentations.Compose([\n",
    "    albumentations.Resize(256,256),\n",
    "    albumentations.RandomCrop(224,224),\n",
    "    albumentations.OneOf([\n",
    "        albumentations.HorizontalFlip(p=1),\n",
    "        albumentations.RandomRotate90(p=1),\n",
    "        albumentations.VerticalFlip(p=1)\n",
    "    ], p=1),\n",
    "    albumentations.OneOf([\n",
    "        albumentations.MotionBlur(p=1),\n",
    "        albumentations.OpticalDistortion(p=1),\n",
    "        albumentations.GaussNoise(p=1)\n",
    "    ], p=1),\n",
    "    ToTensorV2\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_dataset = CatDataset(file_paths=[\"cat.png\"], transform=torchvision_transform)\n",
    "alb_dataset = alb_cat_dataset(file_paths=['cat.png'], transform=albumentations_transform)\n",
    "alb_oneof = alb_cat_dataset(file_paths=[\"cat.png\"], transform=albumentations_transform_oneof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_time = 0\n",
    "for i in range(100):\n",
    "    image, end_ime = cat_dataset[0]\n",
    "    total_time += end_ime\n",
    "print(\"torchvision tiem/image >> \", total_time*10)\n",
    "\n",
    "alb_total_tiem = 0\n",
    "for i in range(100):\n",
    "    alb_image, alb_time = alb_dataset[0]\n",
    "    alb_total_tiem += alb_time\n",
    "print(\"alb time >> \", alb_total_tiem*10)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(transforms.ToPILImage()(image).convert(\"RGB\"))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5 (tags/v3.10.5:f377153, Jun  6 2022, 16:14:13) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e89c93e4c07d4ac8f065cea982a638287e1c61026788fcbbad7e0263e2130583"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
