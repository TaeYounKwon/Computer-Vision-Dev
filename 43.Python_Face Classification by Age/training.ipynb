{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: albumentations in c:\\users\\tempe\\anaconda3\\lib\\site-packages (1.3.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\tempe\\anaconda3\\lib\\site-packages (from albumentations) (1.10.0)\n",
      "Requirement already satisfied: qudida>=0.0.4 in c:\\users\\tempe\\anaconda3\\lib\\site-packages (from albumentations) (0.0.4)\n",
      "Requirement already satisfied: numpy>=1.11.1 in c:\\users\\tempe\\anaconda3\\lib\\site-packages (from albumentations) (1.23.5)\n",
      "Requirement already satisfied: scikit-image>=0.16.1 in c:\\users\\tempe\\anaconda3\\lib\\site-packages (from albumentations) (0.19.3)\n",
      "Requirement already satisfied: opencv-python-headless>=4.1.1 in c:\\users\\tempe\\anaconda3\\lib\\site-packages (from albumentations) (4.7.0.68)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\tempe\\anaconda3\\lib\\site-packages (from albumentations) (6.0)\n",
      "Requirement already satisfied: scikit-learn>=0.19.1 in c:\\users\\tempe\\anaconda3\\lib\\site-packages (from qudida>=0.0.4->albumentations) (1.2.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\tempe\\anaconda3\\lib\\site-packages (from qudida>=0.0.4->albumentations) (4.4.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\tempe\\appdata\\roaming\\python\\python39\\site-packages (from scikit-image>=0.16.1->albumentations) (23.0)\n",
      "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,!=8.3.0,>=6.1.0 in c:\\users\\tempe\\anaconda3\\lib\\site-packages (from scikit-image>=0.16.1->albumentations) (9.3.0)\n",
      "Requirement already satisfied: networkx>=2.2 in c:\\users\\tempe\\anaconda3\\lib\\site-packages (from scikit-image>=0.16.1->albumentations) (2.8.4)\n",
      "Requirement already satisfied: imageio>=2.4.1 in c:\\users\\tempe\\anaconda3\\lib\\site-packages (from scikit-image>=0.16.1->albumentations) (2.24.0)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in c:\\users\\tempe\\anaconda3\\lib\\site-packages (from scikit-image>=0.16.1->albumentations) (1.4.1)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in c:\\users\\tempe\\anaconda3\\lib\\site-packages (from scikit-image>=0.16.1->albumentations) (2022.10.10)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\tempe\\anaconda3\\lib\\site-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (3.1.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\tempe\\anaconda3\\lib\\site-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (1.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install albumentations\n",
    "import albumentations as A "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch    \n",
    "import torch.nn as nn\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from ex04_customdataset import CustomDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from torchvision import models\n",
    "from timm.loss import LabelSmoothingCrossEntropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from albumentations.pytorch.transforms import ToTensorV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pt_파일명 설정\n",
    "model_try = 8\n",
    "model_names = 'mobilenet_v3'\n",
    "\n",
    "# 하이퍼 파라미터 설정\n",
    "EPOCHS = 20\n",
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE = 64\n",
    "LOSS_FUNCTION = LabelSmoothingCrossEntropy()\n",
    "HALF_PERCENT = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc \n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tempe\\anaconda3\\lib\\site-packages\\albumentations\\augmentations\\transforms.py:1149: FutureWarning: This class has been deprecated. Please use RandomBrightnessContrast\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "### 0. Augmentation (train & valid)\n",
    "train_aug = A.Compose([\n",
    "    # 100퍼(val도 동일 적용)\n",
    "    A.RandomCrop(width= 200, height= 200),\n",
    "    #A.CenterCrop(width=200, height=200),\n",
    "    # 50퍼\n",
    "    A.RandomRotate90(p=HALF_PERCENT),\n",
    "    A.VerticalFlip(p=HALF_PERCENT),\n",
    "    A.HorizontalFlip(p=HALF_PERCENT),\n",
    "    A.RandomBrightness(limit=0.2, p=HALF_PERCENT),\n",
    "    A.RGBShift(r_shift_limit=10, g_shift_limit=10, b_shift_limit=10, p = HALF_PERCENT),\n",
    "    # 30퍼(예시에 p값을 0.3주는게 많아서 0.3으로 설정)\n",
    "    A.ShiftScaleRotate(shift_limit= 0.05, scale_limit= 0.06, rotate_limit=20, p= 0.3),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std= (0.229, 0.224, 0.225)),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "valid_aug = A.Compose([\n",
    "    A.CenterCrop(width= 200, height= 200),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std= (0.229, 0.224, 0.225)),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "## visual augmentation\n",
    "def visualize_augmentation(dataset, idx = 0, cols= 5):\n",
    "    dataset = copy.deepcopy(dataset)\n",
    "    samples = 5\n",
    "    dataset.transform = A.Compose([t for t in dataset.transform if not isinstance(\n",
    "        t, (A.Normalize, ToTensorV2)\n",
    "    )])\n",
    "    rows = samples // cols\n",
    "    figure, ax = plt.subplots(nrows= rows, ncols= cols, figsize=(12,6))\n",
    "\n",
    "    for i in range(samples):\n",
    "        image, _ = dataset[idx]\n",
    "        ax.ravel()[i].imshow(image)\n",
    "        ax.ravel()[i].set_axis_off()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# visualize_augmentation(train_dataset)\n",
    "# visualize_augmentation(train_dataset)\n",
    "# visualize_augmentation(train_dataset)\n",
    "# visualize_augmentation(train_dataset)\n",
    "# visualize_augmentation(train_dataset)\n",
    "# visualize_augmentation(train_dataset)\n",
    "# visualize_augmentation(valid_dataset)\n",
    "# visualize_augmentation(test_dataset)\n",
    "\n",
    "### 3. model build\n",
    "\n",
    "\n",
    "# model1 = models.swin_t(weights='IMAGENET1K_V1')\n",
    "# model1.head = nn.Linear(in_features=768, out_features=3)\n",
    "# model1.to(device)\n",
    "\n",
    "# model2 = torch.hub.load('facebookresearch/deit:main',\n",
    "#                    'deit_tiny_patch16_224', pretrained=True)\n",
    "\n",
    "# model2.fc = nn.Linear(in_features=192, out_features=2)\n",
    "# model2.to(device)\n",
    "\n",
    "# model3 = models.__dict__[\"resnet18\"](pretrained= True)\n",
    "# model3.fc = nn.Linear(in_features= 512, out_features= 3)\n",
    "# model3.to(device)\n",
    "\n",
    "\n",
    "# model5 = models.__dict__[\"shufflenet_v2_x0_5\"](pretrained= True)\n",
    "# model5.fc = nn.Linear(in_features= 4096, out_features= 3)\n",
    "# model5.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch[1 / 0], loss2.249: 100%|\u001b[32m██████████\u001b[0m| 152/152 [00:32<00:00,  4.61it/s]\n",
      "100%|\u001b[31m██████████\u001b[0m| 17/17 [00:03<00:00,  5.13it/s]\n",
      "epoch [1/20] train loss : 2.216 train_acc : 0.518 val_acc : 0.565\n",
      "train epoch[2 / 1], loss1.900: 100%|\u001b[32m██████████\u001b[0m| 152/152 [00:29<00:00,  5.19it/s]\n",
      "100%|\u001b[31m██████████\u001b[0m| 17/17 [00:03<00:00,  5.02it/s]\n",
      "epoch [2/20] train loss : 1.892 train_acc : 0.598 val_acc : 0.573\n",
      "train epoch[3 / 2], loss1.710: 100%|\u001b[32m██████████\u001b[0m| 152/152 [00:29<00:00,  5.22it/s]\n",
      "100%|\u001b[31m██████████\u001b[0m| 17/17 [00:03<00:00,  5.05it/s]\n",
      "epoch [3/20] train loss : 1.810 train_acc : 0.641 val_acc : 0.647\n",
      "train epoch[4 / 3], loss1.702: 100%|\u001b[32m██████████\u001b[0m| 152/152 [00:29<00:00,  5.20it/s]\n",
      "100%|\u001b[31m██████████\u001b[0m| 17/17 [00:03<00:00,  5.16it/s]\n",
      "epoch [4/20] train loss : 1.756 train_acc : 0.663 val_acc : 0.589\n",
      "train epoch[5 / 4], loss1.761: 100%|\u001b[32m██████████\u001b[0m| 152/152 [00:29<00:00,  5.17it/s]\n",
      "100%|\u001b[31m██████████\u001b[0m| 17/17 [00:03<00:00,  5.19it/s]\n",
      "epoch [5/20] train loss : 1.734 train_acc : 0.671 val_acc : 0.562\n",
      "train epoch[6 / 5], loss1.407: 100%|\u001b[32m██████████\u001b[0m| 152/152 [00:29<00:00,  5.23it/s]\n",
      "100%|\u001b[31m██████████\u001b[0m| 17/17 [00:03<00:00,  5.16it/s]\n",
      "epoch [6/20] train loss : 1.697 train_acc : 0.680 val_acc : 0.663\n",
      "train epoch[7 / 6], loss1.709: 100%|\u001b[32m██████████\u001b[0m| 152/152 [00:29<00:00,  5.19it/s]\n",
      "100%|\u001b[31m██████████\u001b[0m| 17/17 [00:03<00:00,  4.88it/s]\n",
      "epoch [7/20] train loss : 1.676 train_acc : 0.698 val_acc : 0.679\n",
      "train epoch[8 / 7], loss1.543: 100%|\u001b[32m██████████\u001b[0m| 152/152 [00:29<00:00,  5.11it/s]\n",
      "100%|\u001b[31m██████████\u001b[0m| 17/17 [00:03<00:00,  4.77it/s]\n",
      "epoch [8/20] train loss : 1.654 train_acc : 0.708 val_acc : 0.703\n",
      "train epoch[9 / 8], loss1.772: 100%|\u001b[32m██████████\u001b[0m| 152/152 [00:30<00:00,  4.94it/s]\n",
      "100%|\u001b[31m██████████\u001b[0m| 17/17 [00:03<00:00,  5.09it/s]\n",
      "epoch [9/20] train loss : 1.630 train_acc : 0.722 val_acc : 0.702\n",
      "train epoch[10 / 9], loss1.541: 100%|\u001b[32m██████████\u001b[0m| 152/152 [00:31<00:00,  4.90it/s]\n",
      "100%|\u001b[31m██████████\u001b[0m| 17/17 [00:03<00:00,  5.04it/s]\n",
      "epoch [10/20] train loss : 1.624 train_acc : 0.722 val_acc : 0.676\n",
      "train epoch[11 / 10], loss1.554: 100%|\u001b[32m██████████\u001b[0m| 152/152 [00:30<00:00,  5.05it/s]\n",
      "100%|\u001b[31m██████████\u001b[0m| 17/17 [00:03<00:00,  5.33it/s]\n",
      "epoch [11/20] train loss : 1.604 train_acc : 0.726 val_acc : 0.689\n",
      "train epoch[12 / 11], loss1.587: 100%|\u001b[32m██████████\u001b[0m| 152/152 [00:29<00:00,  5.18it/s]\n",
      "100%|\u001b[31m██████████\u001b[0m| 17/17 [00:03<00:00,  5.28it/s]\n",
      "epoch [12/20] train loss : 1.595 train_acc : 0.736 val_acc : 0.656\n",
      "train epoch[13 / 12], loss1.584: 100%|\u001b[32m██████████\u001b[0m| 152/152 [00:29<00:00,  5.19it/s]\n",
      "100%|\u001b[31m██████████\u001b[0m| 17/17 [00:03<00:00,  5.15it/s]\n",
      "epoch [13/20] train loss : 1.585 train_acc : 0.742 val_acc : 0.667\n",
      "train epoch[14 / 13], loss1.555: 100%|\u001b[32m██████████\u001b[0m| 152/152 [00:29<00:00,  5.19it/s]\n",
      "100%|\u001b[31m██████████\u001b[0m| 17/17 [00:03<00:00,  5.19it/s]\n",
      "epoch [14/20] train loss : 1.567 train_acc : 0.747 val_acc : 0.701\n",
      "train epoch[15 / 14], loss1.492: 100%|\u001b[32m██████████\u001b[0m| 152/152 [00:29<00:00,  5.19it/s]\n",
      "100%|\u001b[31m██████████\u001b[0m| 17/17 [00:03<00:00,  5.17it/s]\n",
      "epoch [15/20] train loss : 1.550 train_acc : 0.752 val_acc : 0.644\n",
      "train epoch[16 / 15], loss1.497: 100%|\u001b[32m██████████\u001b[0m| 152/152 [00:29<00:00,  5.18it/s]\n",
      "100%|\u001b[31m██████████\u001b[0m| 17/17 [00:03<00:00,  5.22it/s]\n",
      "epoch [16/20] train loss : 1.546 train_acc : 0.752 val_acc : 0.715\n",
      "train epoch[17 / 16], loss1.487: 100%|\u001b[32m██████████\u001b[0m| 152/152 [00:29<00:00,  5.18it/s]\n",
      "100%|\u001b[31m██████████\u001b[0m| 17/17 [00:03<00:00,  5.34it/s]\n",
      "epoch [17/20] train loss : 1.537 train_acc : 0.757 val_acc : 0.703\n",
      "train epoch[18 / 17], loss1.573: 100%|\u001b[32m██████████\u001b[0m| 152/152 [00:29<00:00,  5.18it/s]\n",
      "100%|\u001b[31m██████████\u001b[0m| 17/17 [00:03<00:00,  5.26it/s]\n",
      "epoch [18/20] train loss : 1.530 train_acc : 0.762 val_acc : 0.649\n",
      "train epoch[19 / 18], loss1.483: 100%|\u001b[32m██████████\u001b[0m| 152/152 [00:29<00:00,  5.22it/s]\n",
      "100%|\u001b[31m██████████\u001b[0m| 17/17 [00:03<00:00,  5.09it/s]\n",
      "epoch [19/20] train loss : 1.515 train_acc : 0.771 val_acc : 0.676\n",
      "train epoch[20 / 19], loss1.663: 100%|\u001b[32m██████████\u001b[0m| 152/152 [00:29<00:00,  5.20it/s]\n",
      "100%|\u001b[31m██████████\u001b[0m| 17/17 [00:03<00:00,  5.05it/s]\n",
      "epoch [20/20] train loss : 1.517 train_acc : 0.766 val_acc : 0.665\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "### 1. Loading Classification Dataset\n",
    "train_dataset = CustomDataset(\"./dataset/train\" , transform= train_aug)\n",
    "valid_dataset = CustomDataset(\"./dataset/val\"   , transform= valid_aug)\n",
    "\n",
    "### 2. Data Loader\n",
    "train_loader = DataLoader(train_dataset, batch_size= BATCH_SIZE, shuffle= True , num_workers= 2, pin_memory= True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size= BATCH_SIZE, shuffle= False, num_workers= 2, pin_memory= True)\n",
    "\n",
    "\n",
    "\n",
    "model_list = []\n",
    "model= models.mobilenet_v3_large(pretrained=True)\n",
    "# model = models.__dict__[\"resnet152\"](pretrained= True)\n",
    "model.fc = nn.Linear(in_features = 1028, out_features = 6)\n",
    "model.to(device)\n",
    "\n",
    "model_list= [model]\n",
    "\n",
    "#### 4 epoch, optim loss\n",
    "epochs = EPOCHS\n",
    "loss_function = LOSS_FUNCTION\n",
    "\n",
    "best_val_acc = 0.0\n",
    "\n",
    "train_steps = len(train_loader)\n",
    "valid_steps = len(valid_loader)\n",
    "\n",
    "############ 수정하기 ####\n",
    "for index, model in enumerate(model_list):\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr= LEARNING_RATE)\n",
    "    # optimizer = torch.optim.Adam(model.parameters(), lr= LEARNING_RATE)\n",
    "\n",
    "    save_path = f'./best_{model_try}.pt'\n",
    "    dfForAccuracy = pd.DataFrame(index=list(range(epochs)),\n",
    "                                columns=['Epoch', 'Accuracy', 'Loss'])\n",
    "\n",
    "    if os.path.exists(save_path) :\n",
    "        best_val_acc = max(pd.read_csv(f'./{model_names}_{model_try}.csv')['Accuracy'].tolist())\n",
    "        model.load_state_dict(torch.load(save_path))\n",
    "\n",
    "    for epoch in range(epochs) :\n",
    "        runing_loss = 0\n",
    "        val_acc = 0\n",
    "        train_acc = 0\n",
    "\n",
    "        model.train()\n",
    "        train_bar = tqdm(train_loader, file=sys.stdout, colour='green')\n",
    "        for step, data in enumerate(train_bar) :\n",
    "            images , labels = data\n",
    "            images , labels = images.to(device) , labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = loss_function(outputs, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            train_acc += (torch.argmax(outputs, dim=1) == labels).sum().item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            runing_loss += loss.item()\n",
    "            train_bar.desc = f\"train epoch[{epoch+1} / {epoch}], loss{loss.data:.3f}\"\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad() :\n",
    "            valid_bar = tqdm(valid_loader, file=sys.stdout, colour='red')\n",
    "            for data in valid_bar :\n",
    "                images, labels = data\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                val_acc += (torch.argmax(outputs, dim=1) == labels).sum().item()\n",
    "\n",
    "        val_accuracy = val_acc / len(valid_dataset)\n",
    "        train_accuracy = train_acc / len(train_dataset)\n",
    "\n",
    "        print(f\"epoch [{epoch+1}/{epochs}]\"\n",
    "            f\" train loss : {(runing_loss / train_steps):.3f} \"\n",
    "            f\"train_acc : {train_accuracy:.3f} val_acc : {val_accuracy:.3f}\"\n",
    "        )\n",
    "\n",
    "        dfForAccuracy.loc[epoch, 'Epoch']    = epoch + 1\n",
    "        dfForAccuracy.loc[epoch, 'Accuracy'] = round(val_accuracy, 4) * 100\n",
    "        dfForAccuracy.loc[epoch, 'Loss']     = round( (runing_loss / train_steps), 4)\n",
    "        \n",
    "        if val_accuracy > best_val_acc :\n",
    "            best_val_acc = val_accuracy\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "\n",
    "        if epoch == epochs - 1 :\n",
    "            dfForAccuracy.to_csv(f\"./{model_names}_{model_try}.csv\" , index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc_function(correct, total) :\n",
    "    acc = correct / total * 100\n",
    "    return acc\n",
    "\n",
    "def test(model, data_loader, device) :\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (image, label) in enumerate(data_loader) :\n",
    "            images, labels = image.to(device), label.to(device)\n",
    "            output = model(images)\n",
    "            _, argmax = torch.max(output, 1)\n",
    "            total += images.size(0)\n",
    "            correct += (labels == argmax).sum().item()\n",
    "        acc = acc_function(correct, total)\n",
    "        print(f\"acc >> {acc}%\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc >> 66.99604743083005%\n"
     ]
    }
   ],
   "source": [
    "test_aug = A.Compose([\n",
    "        A.CenterCrop(width= 200, height= 200),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std= (0.229, 0.224, 0.225)),\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "\n",
    "test_dataset = CustomDataset(\"./dataset/test\" , transform= test_aug)\n",
    "test_loader  = DataLoader(test_dataset, batch_size= 1, shuffle= False, num_workers= 2, pin_memory= True)\n",
    "\n",
    "###### 수정해야 할 부분 !!!!!!!!!!!!!!!!!!!\n",
    "model = models.mobilenet_v3_large(pretrained=False)\n",
    "model.fc = nn.Linear(in_features = 1028, out_features = 6)\n",
    "model.load_state_dict(torch.load(f'./best_8.pt', map_location=device))\n",
    "model.to(device)\n",
    "\n",
    "test(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ec1db613bd433334d4526401b52388526e3088498c79e7df5f78f6c21e8ddf15"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
