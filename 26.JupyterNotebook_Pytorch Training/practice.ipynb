{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### RuntimeError: Input type (torch.cuda.ByteTensor) and weight type (torch.cuda.FloatTensor) should be the same\n",
    "\n",
    "- 입력이 0~255로 표현되는 Byte(uint8)형태이고, 반면 모델의 가중치는 Float32이기 때문에 발생하는 에러입니다.\n",
    "\n",
    "- 해결방법 : customdataset 에서 image = image.float() 형태로 변경하기 \n",
    "\n",
    "### Target size (torch.Size([64])) must be the same as input size (torch.Size([64, 1]))\n",
    "\n",
    "- 타겟 차원수가 맞지 않아서 발생하는 문제 아마 이진분류 loss 함수를 사용하다보니 생기는 문제\n",
    "\n",
    "- 해결방법 : train, val 함수에서  targets = targets.unsqueeze(1) 차원 추가\n",
    "\n",
    "### RuntimeError: result type Float can't be cast to the desired output type Long\n",
    "\n",
    "- BCEWithLogitsLoss로 다중 레이블 이진 분류를 수행하지만 RunTimeError \"RuntimeError: 결과 유형 Float를 원하는 출력 유형 Long으로 캐스트할 수 없습니다\"가 발생합니다.\n",
    "\n",
    "- 해결방법 : loss 계산하는 부분에 targets.float() 변경하기 \n",
    "\n",
    "- 모델 가중치가 Float32 인데 들어오는건 Long 타입이라문제가 발생한거니 Float형태로 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import os\n",
    "import copy\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import albumentations as A\n",
    "import numpy as np\n",
    "\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# m1제품 해당하는것\n",
    "# device = torch.device('mps:0' if torch.backends.mps.is_available() else 'cpu')\n",
    "# print('device', device)\n",
    "\n",
    "# print(f\"PyTorch version:{torch.__version__}\")  # 1.12.1 이상 True 여야 합니다.\n",
    "# print(f\"MPS 장치를 지원하도록 build 되었는지: {torch.backends.mps.is_built()}\")\n",
    "# print(f\"MPS 장치가 사용 가능한지: {torch.backends.mps.is_available()}\")  # True 여야 합니다.\n",
    "\n",
    "# 엔비디아 GPU 사용자는 여기 주석 풀고 device 사용\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class catvsdogDataset(Dataset):\n",
    "    def __init__(self, image_file_path, transform=None):\n",
    "        self.image_file_paths = glob.glob(\n",
    "            os.path.join(image_file_path, \"*\", \"*.jpg\"))\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # image loader\n",
    "        image_path = self.image_file_paths[index]\n",
    "        print(image_path)\n",
    "        \n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # converter BGR -> RGB\n",
    "        \n",
    "        # label\n",
    "        label_temp = image_path.split(\"\\\\\")\n",
    "        label_temp = label_temp[1]\n",
    "        label = 0\n",
    "        print(label_temp)\n",
    "        \n",
    "        if \"cat\" == label_temp:\n",
    "            label = 0\n",
    "        elif \"dog\" == label_temp:\n",
    "            label = 1\n",
    "        print(image_path, label)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image=image)[\"image\"]\n",
    "\n",
    "        return image, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_file_paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augmentation\n",
    "train_transform = A.Compose([\n",
    "    A.Resize(height=224, width=224),\n",
    "    A.RGBShift(r_shift_limit=15, g_shift_limit=15, b_shift_limit=15, p=1.0),\n",
    "    ToTensorV2()\n",
    "])\n",
    "val_transform = A.Compose([\n",
    "    A.Resize(height=224, width=224),\n",
    "    ToTensorV2()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset 구성\n",
    "train_dataset = catvsdogDataset(\"./dataset/train/\", transform=train_transform)\n",
    "val_dataset = catvsdogDataset(\"./dataset/val/\", transform=val_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize augmentation\n",
    "def visualize_augmentation(dataset, idx=0, samples=10, cols=5):\n",
    "    dataset = copy.deepcopy(dataset)\n",
    "    dataset.transform = A.Compose([\n",
    "        t for t in dataset.transform if not isinstance(t, (ToTensorV2))])\n",
    "    rows = samples // cols\n",
    "    figure, ax = plt.subplots(nrows=rows, ncols=cols, figsize=(12, 6))\n",
    "    for i in range(samples):\n",
    "        image, _ = dataset[idx]\n",
    "        ax.ravel()[i].imshow(image)\n",
    "        ax.ravel()[i].set_axis_off()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가\n",
    "def calculate_accuracy(output, target) :\n",
    "    output = target.sigmoid(output) >= 0.5\n",
    "    target = target == 1.0 # a.item()이면 a의 텐서값을 뽑아낼 수 있음\n",
    "    return torch.true_divide((target==output).sum(dim=0), output.size(0)).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricMonitor :\n",
    "    def __init__(self, float_precisipon=3) :\n",
    "        self.float_precisipon = float_precisipon\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self) :\n",
    "        self.metrics = defaultdict(lambda:{\"val\" : 0, \"count\" : 0, \"avg\" : 0})\n",
    "    \n",
    "    def update(self, metric_name, val) :\n",
    "        metric = self.metrics[metric_name]\n",
    "        metric[\"val\"] += val\n",
    "        metric[\"count\"] += 1\n",
    "        metric[\"avg\"] = metric[\"val\"] / metric[\"count\"]\n",
    "\n",
    "    # 들어있는 내용을 문자열로 반환해라\n",
    "    def __str__(self) :\n",
    "        return \" | \".join(\n",
    "            [\n",
    "            \"{metric_name} : {avg : .{float_precisipon}f}\".format(\n",
    "                metric_name=metric_name, avg=metric[\"avg\"], float_precisipon=self.float_precisipon\n",
    "                )\n",
    "            for (metric_name, metric) in self.metrics.items()\n",
    "            ]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"model\" : \"resnet18\",\n",
    "    \"device\" : \"cuda\", # 자기 환경에 맞는 device 사용. ex) cuda, cpu, mps....\n",
    "    \"lr\" : 0.001,\n",
    "    \"batch_size\" : 64,\n",
    "    \"num_workers\" : 4,\n",
    "    \"epoch\" : 10\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [21], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m model \u001b[39m=\u001b[39m models\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m[params[\u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m]](pretrained\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m      3\u001b[0m model\u001b[39m.\u001b[39mfc \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLinear(\u001b[39m512\u001b[39m, \u001b[39m2\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m model \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mto(params[\u001b[39m\"\u001b[39;49m\u001b[39mdevice\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[0;32m      5\u001b[0m \u001b[39mprint\u001b[39m(model)\n",
      "File \u001b[1;32mc:\\Users\\tempe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:989\u001b[0m, in \u001b[0;36mModule.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    985\u001b[0m         \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    986\u001b[0m                     non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[0;32m    987\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, non_blocking)\n\u001b[1;32m--> 989\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(convert)\n",
      "File \u001b[1;32mc:\\Users\\tempe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:641\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    639\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[0;32m    640\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[1;32m--> 641\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[0;32m    643\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    644\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    645\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    646\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    651\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    652\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\tempe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:664\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    660\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    661\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    662\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    663\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m--> 664\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[0;32m    665\u001b[0m should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    666\u001b[0m \u001b[39mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[1;32mc:\\Users\\tempe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:987\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    984\u001b[0m \u001b[39mif\u001b[39;00m convert_to_format \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m t\u001b[39m.\u001b[39mdim() \u001b[39min\u001b[39;00m (\u001b[39m4\u001b[39m, \u001b[39m5\u001b[39m):\n\u001b[0;32m    985\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    986\u001b[0m                 non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[1;32m--> 987\u001b[0m \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39;49mto(device, dtype \u001b[39mif\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_floating_point() \u001b[39mor\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_complex() \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m, non_blocking)\n",
      "File \u001b[1;32mc:\\Users\\tempe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\cuda\\__init__.py:221\u001b[0m, in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    217\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m    218\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    219\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mmultiprocessing, you must use the \u001b[39m\u001b[39m'\u001b[39m\u001b[39mspawn\u001b[39m\u001b[39m'\u001b[39m\u001b[39m start method\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    220\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(torch\u001b[39m.\u001b[39m_C, \u001b[39m'\u001b[39m\u001b[39m_cuda_getDeviceCount\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m--> 221\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTorch not compiled with CUDA enabled\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    222\u001b[0m \u001b[39mif\u001b[39;00m _cudart \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    223\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\n\u001b[0;32m    224\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "# model loader\n",
    "model = models.__dict__[params[\"model\"]](pretrained=True)\n",
    "model.fc = nn.Linear(512, 2)\n",
    "model = model.to(params[\"device\"])\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=params[\"lr\"])\n",
    "train_loader = DataLoader(train_dataset, batch_size=params[\"batch_size\"], shuffle=True) # num_workers=params[\"num_workers\"]\n",
    "val_loader = DataLoader(val_dataset, batch_size=params[\"batch_size\"], shuffle=False) # num_workers=params[\"num_workers\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "def save_model(model, save_dir, file_name='last.pt') :\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    output_path = os.path.join(save_dir, file_name) # ./weights/last.pt\n",
    "    if isinstance(model, nn.DataParallel) : # GPU 2개 이상일 때\n",
    "        print(\"multi GPU activate\")\n",
    "        torch.save(model.module.state_dict(), output_path)\n",
    "    else : # GPU 1개일 때\n",
    "        print(\"single GPU activate\")\n",
    "        torch.save(model.state_dict(),output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimizer, epoch, params, save_dir) :\n",
    "    metric_monitor = MetricMonitor()\n",
    "    model.train()\n",
    "    stream = tqdm(train_loader)\n",
    "    for i, (image, target) in enumerate(train_loader) :\n",
    "        images = image.to(params[\"device\"])\n",
    "        targets = target.to(params[\"device\"])\n",
    "\n",
    "        output = model(images)\n",
    "        loss = criterion(output, targets) # tensor값으로 튀어나옴\n",
    "        accuracy = calculate_accuracy(output, targets)\n",
    "        metric_monitor.update(\"Loss\", loss.item())\n",
    "        metric_monitor.update(\"Accuracy\", accuracy)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        stream.set_description(\n",
    "            \"Epoch : {epoch}. | Train. {metric_monitor}\".format(epoch=epoch, metric_monitor=metric_monitor)\n",
    "        )\n",
    "    \n",
    "    save_model(model, save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(val_loader, model, criterion, epoch, params) :\n",
    "    metric_monitor = MetricMonitor()\n",
    "    model.eval() # 모델을 평가 모드로 전환\n",
    "    stream = tqdm(val_loader)\n",
    "    with torch.no_grad() : # 학습 더이상 안 할 거임\n",
    "        for i, (image, target) in enumerate(val_loader) :\n",
    "                images = image.to(params[\"device\"])\n",
    "                targets = target.to(params[\"device\"])\n",
    "\n",
    "                output = model(images)\n",
    "                loss = criterion(output, targets) # tensor값으로 튀어나옴\n",
    "                accuracy = calculate_accuracy(output, targets)\n",
    "                metric_monitor.update(\"Loss\", loss.item())\n",
    "                metric_monitor.update(\"Accuracy\", accuracy)\n",
    "\n",
    "                stream.set_description(\n",
    "                    \"Epoch : {epoch}. val. {metric_monitor}\".format(epoch=epoch, metric_monitor=metric_monitor)\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = \"./weights\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, params[\"epoch\"]+1) :\n",
    "    train(train_loader, model, criterion, optimizer, epoch, params, save_dir)\n",
    "    validate(val_loader, model, criterion, optimizer, epoch, params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e89c93e4c07d4ac8f065cea982a638287e1c61026788fcbbad7e0263e2130583"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
